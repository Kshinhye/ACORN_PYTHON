# 단순선형회귀 모델
# 기본적인 결정론적 선형회귀 방법 : 독립변수에 대해 대응하는 종속변수와 유사한 예측값은 출력하는 함수 f(x)를 찾는 작업이다.

import pandas as pd

df=pd.read_csv("../testdata/drinking_water.csv")
# print(df)
# print(df.corr())

import statsmodels.formula.api as smf

#적절성이 만족도에 영향을 준다라는 가정하에 모델 생성(사람이 생각한거 정말로 확인하려면 p값 확인해야함)
#합습할 때 fit()의 파라미터값이 중요한것은 딥러닝이다.
model=smf.ols(formula='만족도 ~ 적절성', data=df).fit()

#생성된 모델의 요약결과를 반환한다. 능력치를 확인할 수 있다.
print(model.summary())

#기울기(0.7393)를 표준오차(0.038)로 나눠준것이 T값(19.340)
#F-statistic: T값(19.340 거듭제곱)으로 얻어낸 F값(374.0)
#Prob (F-statistic) : 모델의 성능을 파악한 p-value
# 유의수준이 0.05보다작다.회귀모델은 적합하다.  / 독립변수는 종속변수에 통계적으로 유의하다

#                             OLS Regression Results                            
# ==============================================================================
# Dep. Variable:                    만족도   R-squared:(단순선형회귀)             0.588 #이 모델은 58.8%의 설명력을 가지고있다
# Model:                            OLS   Adj. R-squared:(다중선형회귀)         0.586
# Method:                 Least Squares   F-statistic:                     374.0
# Date:                Tue, 15 Nov 2022   Prob (F-statistic):           2.24e-52
# Time:                        11:24:45   Log-Likelihood:                -207.44
# No. Observations:                 264   AIC:                             418.9
# Df Residuals:                     262   BIC:                             426.0
# Df Model:                           1                                         
# Covariance Type:            nonrobust                                         

# ==============================================================================
#                  coef    std err          t      P>|t|      [0.025      0.975]
# ------------------------------------------------------------------------------
# Intercept  0.7789 (절편)    0.124      6.273      0.000       0.534       1.023
# 적절성       0.7393 (기울기) / 0.038  =  19.340      0.000       0.664       0.815
# ==============================================================================

#표본평균들의 표준편차 : 스탠다드 에러
#표준오차가 작으면 모집단과 표본집단이랑 차이가 작다는거야

#기울기와 절편은 최소자승법(최소제곱법)이용 | 추세선을 그어줄 수 있다.
#최소자승법은 사용하면 데이터가 밀도가 높거나 낮거나 상관계수의 크기에 상관없이 패턴이 같으면 기울기와 절편값이 똑같이 나온다
#(R-squared:결정계수/설명력 :x가 종속변수 y의 분산을 설명하는 비율)
#표준오차가 작다 : 분산의 설명력이 높다. (유의하다라고 말 할 수 있다)
#표준오차가 크다 : 분산의 설명력이 낮다. 상관계수(r)가 약하다. (이 모델은 유의하지않다라고 말 할 수 있다.) 
#결정계수: R^2=r^2 | 1- SSE(회귀식 추정 y의 편차제곱의 합,설명된변동값)/SST(개별 y의 편차제곱의 합,총변동값) | 설명된 분산/종속변수의 전체분산
#SST(총변동) = SSE(설명된변동) + SSR(설명안된 변동)

# ==============================================================================
# Omnibus: (유의성)               11.674   Durbin-Watson: (잔차의 독립성확인)     2.185 #2에 가까울수록 독립적이다(좋은거)
# Prob(Omnibus):                 0.003   Jarque-Bera (JB): (적합도검정)      16.003 #JB두개 같이보기
# Skew: (왜도)치우침 정도            -0.328   Prob(JB): (오차의 정규성)          0.000335 #0에 가까울수록 정규분포에 가깝다
# Kurtosis:(첨도)                 4.012   Cond. No.                         13.4
# ==============================================================================
#Skew, Kurtosis와 가까운 애들 Jarque-Bera (JB),  Prob(JB)


#하나하나씩 보고싶다면
print('---회귀계수--- \n', model.params)
print('---결정계수--- \n', model.rsquared)
print('---유의확률--- \n', model.pvalues)
print('---예측값--- \n', model.predict()[:5])
print('---실제값--- \n', df.만족도[:5].values)


print()
new_df=pd.DataFrame({'적절성':[4,3,2,1,]}) #적절성이 4일떄, 3일떄, 2일 때,1일 때
new_pred=model.predict(new_df)
print('예측결과:',new_df)

















